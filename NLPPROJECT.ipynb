{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CELL 1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p data/policies\n",
        "!cp /content/drive/MyDrive/CU_Policies/*.pdf data/policies/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2AnzwOEciqr",
        "outputId": "93a833ab-6ac6-4855-d5c8-0c63b1a77940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbIYJcawb63D",
        "outputId": "d0a14867-7a53-4788-e314-330901279dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# CELL 2: Fixed Installation (Latest Compatible Versions)\n",
        "!pip install -q \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-huggingface \\\n",
        "    faiss-cpu \\\n",
        "    pypdf \\\n",
        "    streamlit \\\n",
        "    pyngrok \\\n",
        "    python-dotenv \\\n",
        "    langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Updated app.py (Compatible with Latest LangChain)\n",
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Page config\n",
        "# -------------------------------------------------\n",
        "st.set_page_config(page_title=\"CU Policy Chatbot\", layout=\"centered\")\n",
        "st.title(\"Chandigarh University Student Help Chatbot\")\n",
        "st.caption(\"Ask anything about CU policies – powered by Llama-3-8B via Groq\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Load environment variables (GROQ_API_KEY)\n",
        "# -------------------------------------------------\n",
        "\n",
        "GROQ_API_KEY = \"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\"\n",
        "if not GROQ_API_KEY:\n",
        "    st.error(\"❌ GROQ_API_KEY not found. Add it in **Secrets** (left pane) or create a `.env` file.\")\n",
        "    st.stop()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Build FAISS index – cached once\n",
        "# -------------------------------------------------\n",
        "@st.cache_resource(show_spinner=\"Loading PDFs & building index…\")\n",
        "def build_vectorstore():\n",
        "    folder = \"data/policies\"\n",
        "    if not os.path.exists(folder):\n",
        "        st.error(\"❌ PDF folder not found! Run the Drive mount cell first.\")\n",
        "        st.stop()\n",
        "\n",
        "    docs = []\n",
        "    for file in os.listdir(folder):\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(os.path.join(folder, file))\n",
        "            docs.extend(loader.load())\n",
        "\n",
        "    if not docs:\n",
        "        st.error(\"❌ No PDFs found in data/policies!\")\n",
        "        st.stop()\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_documents(chunks, embedder)\n",
        "    return db, embedder\n",
        "\n",
        "db, embedder = build_vectorstore()\n",
        "st.success(f\"✅ Indexed {len(db.index_to_docstore_id)} chunks from {len(os.listdir('data/policies'))} PDFs!\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. LLM – lazy, API-based\n",
        "# -------------------------------------------------\n",
        "@st.cache_resource\n",
        "def get_llm():\n",
        "    return ChatGroq(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        temperature=0.2,\n",
        "        groq_api_key=GROQ_API_KEY,\n",
        "    )\n",
        "\n",
        "llm = get_llm()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. UI – ask a question\n",
        "# -------------------------------------------------\n",
        "query = st.text_input(\"Your question about CU policies\", placeholder=\"e.g. What is the late-fee policy?\")\n",
        "\n",
        "if st.button(\"Ask\") and query.strip():\n",
        "    with st.spinner(\"Searching policies…\"):\n",
        "        docs = db.similarity_search(query, k=5)\n",
        "        context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"Use **only** the following context to answer the question factually.\n",
        "If the answer isn't in the context, say: \"I couldn't find that information in the policies.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    with st.spinner(\"Generating answer with Llama-3…\"):\n",
        "        try:\n",
        "            answer = llm.invoke(prompt).content.strip()\n",
        "            st.success(\"✅ Answer\")\n",
        "            st.markdown(answer)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ LLM error: {e}. Check your Groq API key.\")\n",
        "else:\n",
        "    if query.strip():\n",
        "        st.info(\"Click 'Ask' to get your answer!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DZTkew2csmr",
        "outputId": "42afee30-dd6d-4a6f-d602-06a3e41977af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5\n",
        "import subprocess, time, os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ---- Kill any old processes -------------------------------------------------\n",
        "!pkill -f streamlit 2>/dev/null\n",
        "!pkill -f ngrok    2>/dev/null\n",
        "\n",
        "# ---- Start Streamlit (background) -------------------------------------------\n",
        "proc = subprocess.Popen([\n",
        "    \"streamlit\", \"run\", \"app.py\",\n",
        "    \"--server.port\", \"8501\",\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.headless\", \"true\"\n",
        "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "time.sleep(10)   # give Streamlit a moment to boot\n",
        "\n",
        "# ---- ngrok tunnel -----------------------------------------------------------\n",
        "ngrok.kill()    # clean old tunnels\n",
        "ngrok.set_auth_token(\"34uz6DmUmqAglxJz9Sm6lO09V7u_4tY7UyqdYoMk15iyrtQAQ\")\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"\\nYour chatbot is LIVE at:\\n   {public_url}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He7uyBwgc5Qq",
        "outputId": "c8d84efa-20d0-4e35-be2a-a054fbaf7f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n",
            "\n",
            "Your chatbot is LIVE at:\n",
            "   NgrokTunnel: \"https://aciculate-alice-nontumultuous.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n"
          ]
        }
      ]
    }
  ]
}